{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bab4f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:25:32.697519Z",
     "iopub.status.busy": "2025-12-03T06:25:32.697325Z",
     "iopub.status.idle": "2025-12-03T06:25:36.982262Z",
     "shell.execute_reply": "2025-12-03T06:25:36.981522Z"
    },
    "id": "3YfyuMQkggsz",
    "papermill": {
     "duration": 4.289942,
     "end_time": "2025-12-03T06:25:36.984200",
     "exception": false,
     "start_time": "2025-12-03T06:25:32.694258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-tcn-demo\tnfl-big-data-bowl-2026-prediction\r\n",
      "keras_tcn-3.5.6-py3-none-any.whl\r\n",
      "protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/keras-tcn-demo/protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: protobuf\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 6.33.0\r\n",
      "    Uninstalling protobuf-6.33.0:\r\n",
      "      Successfully uninstalled protobuf-6.33.0\r\n",
      "Successfully installed protobuf-5.29.5\r\n",
      "Processing /kaggle/input/keras-tcn-demo/keras_tcn-3.5.6-py3-none-any.whl\r\n",
      "Installing collected packages: keras-tcn\r\n",
      "Successfully installed keras-tcn-3.5.6\r\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Just to check the path and filenames:\n",
    "!ls /kaggle/input\n",
    "!ls /kaggle/input/keras-tcn-demo\n",
    "\n",
    "# 1) Install protobuf from the wheel in dataset\n",
    "!pip install /kaggle/input/keras-tcn-demo/protobuf-5.29.5-*.whl --no-deps\n",
    "\n",
    "# 2) Install keras-tcn from the wheel in dataset\n",
    "!pip install /kaggle/input/keras-tcn-demo/keras_tcn-3.5.6-py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80987eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:25:36.990500Z",
     "iopub.status.busy": "2025-12-03T06:25:36.990008Z",
     "iopub.status.idle": "2025-12-03T06:25:52.567349Z",
     "shell.execute_reply": "2025-12-03T06:25:52.566600Z"
    },
    "id": "sWB1BMWKggs2",
    "papermill": {
     "duration": 15.58166,
     "end_time": "2025-12-03T06:25:52.568475",
     "exception": false,
     "start_time": "2025-12-03T06:25:36.986815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 06:25:38.474401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764743138.655782      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764743138.708035      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN OK, TF: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "from tcn import TCN\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "print(\"TCN OK, TF:\", tf.__version__)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11131e96",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-03T06:25:52.574480Z",
     "iopub.status.busy": "2025-12-03T06:25:52.573882Z",
     "iopub.status.idle": "2025-12-03T06:25:53.049171Z",
     "shell.execute_reply": "2025-12-03T06:25:53.048309Z"
    },
    "id": "Bgc8o-XJggs3",
    "papermill": {
     "duration": 0.479522,
     "end_time": "2025-12-03T06:25:53.050404",
     "exception": false,
     "start_time": "2025-12-03T06:25:52.570882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA exists: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tcn import TCN\n",
    "\n",
    "# =========================\n",
    "# 1. PATHS (KAGGLE VERSION)\n",
    "# =========================\n",
    "\n",
    "# In Kaggle, the competition data is here:\n",
    "DATA = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/train\")\n",
    "print(\"DATA exists:\", DATA.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9d4471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:25:53.056781Z",
     "iopub.status.busy": "2025-12-03T06:25:53.056333Z",
     "iopub.status.idle": "2025-12-03T06:30:31.134938Z",
     "shell.execute_reply": "2025-12-03T06:30:31.134199Z"
    },
    "id": "E4W1t1LDggs4",
    "papermill": {
     "duration": 278.083253,
     "end_time": "2025-12-03T06:30:31.136083",
     "exception": false,
     "start_time": "2025-12-03T06:25:53.052830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train folder exists: True\n",
      "Test_input exists: True\n",
      "Test.csv exists: True\n",
      "inp_all: (4880579, 24) df_output: (562936, 7)\n",
      "X_seq: (46012, 10, 25) Y_seq: (46012, 20, 2)\n",
      "T_IN, F = 10 25 | T_OUT = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764743395.448903      20 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1764743395.449517      20 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764743400.475175      76 service.cc:148] XLA service 0x7e172c045c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764743400.475820      76 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1764743400.475841      76 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1764743401.140175      76 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 27/288\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 26.7551"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764743406.676939      76 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - loss: 10.0450 - val_loss: 1.1210\n",
      "Epoch 2/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3741 - val_loss: 0.8665\n",
      "Epoch 3/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0630 - val_loss: 0.8023\n",
      "Epoch 4/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8974 - val_loss: 0.6992\n",
      "Epoch 5/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8289 - val_loss: 0.6688\n",
      "Epoch 6/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7599 - val_loss: 0.6470\n",
      "Epoch 7/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7364 - val_loss: 0.6139\n",
      "Epoch 8/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6999 - val_loss: 0.6492\n",
      "Epoch 9/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6678 - val_loss: 0.6088\n",
      "Epoch 10/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6549 - val_loss: 0.6069\n",
      "Epoch 11/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6279 - val_loss: 0.6218\n",
      "Epoch 12/12\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6134 - val_loss: 0.6012\n",
      "TCN RMSE (masked, relative) on val: 0.776\n",
      "TCN RMSE (masked, absolute) on val: 0.776\n",
      "RMSE (Offense, abs): 0.602\n",
      "RMSE (Defense, abs): 0.838\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FULL PIPELINE (train + test + submission)\n",
    "# =========================\n",
    "\n",
    "# ---- Paths inside Kaggle ----\n",
    "DATA_ROOT   = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n",
    "DATA_TRAIN  = DATA_ROOT / \"train\"\n",
    "TEST_INPUT  = DATA_ROOT / \"test_input.csv\"\n",
    "TEST_KEYS   = DATA_ROOT / \"test.csv\"\n",
    "\n",
    "print(\"Train folder exists:\", DATA_TRAIN.exists())\n",
    "print(\"Test_input exists:\", TEST_INPUT.exists())\n",
    "print(\"Test.csv exists:\", TEST_KEYS.exists())\n",
    "\n",
    "# =========================\n",
    "# 1. Load + standardize train\n",
    "# =========================\n",
    "def standardize_by_play_direction(df):\n",
    "    left = df[\"play_direction\"].eq(\"left\")\n",
    "    # positions\n",
    "    df.loc[left, \"x\"] = 120 - df.loc[left, \"x\"]\n",
    "    df.loc[left, \"y\"] = 53.3 - df.loc[left, \"y\"]\n",
    "    if \"ball_land_x\" in df:\n",
    "        df.loc[left, \"ball_land_x\"] = 120 - df.loc[left, \"ball_land_x\"]\n",
    "    if \"ball_land_y\" in df:\n",
    "        df.loc[left, \"ball_land_y\"] = 53.3 - df.loc[left, \"ball_land_y\"]\n",
    "    # angles\n",
    "    if \"dir\" in df:\n",
    "        df.loc[left, \"dir\"] = (df.loc[left, \"dir\"] + 180) % 360\n",
    "    if \"o\" in df:\n",
    "        df.loc[left, \"o\"] = (df.loc[left, \"o\"] + 180) % 360\n",
    "    return df\n",
    "\n",
    "def load_inputs_outputs(data_dir: Path):\n",
    "    inp_weeks = []\n",
    "    out_weeks = []\n",
    "    for w in range(1, 19):\n",
    "        week = f\"w{w:02d}\"\n",
    "        df_in = pd.read_csv(data_dir / f\"input_2023_{week}.csv\")\n",
    "        df_in = standardize_by_play_direction(df_in)\n",
    "        df_in[\"week\"] = week\n",
    "        inp_weeks.append(df_in)\n",
    "\n",
    "        df_out = pd.read_csv(data_dir / f\"output_2023_{week}.csv\")\n",
    "        df_out[\"week\"] = week\n",
    "        out_weeks.append(df_out)\n",
    "\n",
    "    inp_all = pd.concat(inp_weeks, ignore_index=True)\n",
    "    df_output = pd.concat(out_weeks, ignore_index=True)\n",
    "    return inp_all, df_output\n",
    "\n",
    "inp_all, df_output = load_inputs_outputs(DATA_TRAIN)\n",
    "print(\"inp_all:\", inp_all.shape, \"df_output:\", df_output.shape)\n",
    "\n",
    "# Map play_direction into output and standardize outputs too\n",
    "dir_map = inp_all[[\"game_id\", \"play_id\", \"play_direction\"]].drop_duplicates()\n",
    "df_output = df_output.merge(dir_map, on=[\"game_id\", \"play_id\"], how=\"left\")\n",
    "df_output = standardize_by_play_direction(df_output)\n",
    "\n",
    "# =========================\n",
    "# 2. Feature engineering (TRAIN)\n",
    "# =========================\n",
    "group_cols_full  = [\"game_id\", \"play_id\", \"frame_id\"]\n",
    "player_key       = [\"game_id\", \"play_id\", \"frame_id\", \"nfl_id\"]\n",
    "\n",
    "base = inp_all.copy().sort_values(player_key)\n",
    "\n",
    "# 2.1 self-based\n",
    "base[[\"dx\", \"dy\"]] = (\n",
    "    base.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[[\"x\", \"y\"]]\n",
    "        .diff()\n",
    "        .fillna(0.0)\n",
    ")\n",
    "base[\"do\"] = (\n",
    "    base.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"o\"]\n",
    "        .diff()\n",
    "        .fillna(0.0)\n",
    ")\n",
    "base[\"dist_to_ball\"] = np.hypot(\n",
    "    base[\"x\"] - base[\"ball_land_x\"],\n",
    "    base[\"y\"] - base[\"ball_land_y\"]\n",
    ")\n",
    "angle_to_ball = np.arctan2(\n",
    "    base[\"ball_land_y\"] - base[\"y\"],\n",
    "    base[\"ball_land_x\"] - base[\"x\"]\n",
    ")\n",
    "base[\"ball_dirx\"] = np.cos(angle_to_ball)\n",
    "base[\"ball_diry\"] = np.sin(angle_to_ball)\n",
    "rad_dir = np.deg2rad(base[\"dir\"])\n",
    "base[\"vx\"] = base[\"s\"] * np.cos(rad_dir)\n",
    "base[\"vy\"] = base[\"s\"] * np.sin(rad_dir)\n",
    "base[[\"ddx\", \"ddy\"]] = (\n",
    "    base.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[[\"dx\", \"dy\"]]\n",
    "        .diff()\n",
    "        .fillna(0.0)\n",
    ")\n",
    "\n",
    "# 2.2 QB features\n",
    "qb = (\n",
    "    base[base[\"player_role\"] == \"Passer\"]\n",
    "    [group_cols_full + [\"x\", \"y\"]]\n",
    "    .rename(columns={\"x\": \"x_qb\", \"y\": \"y_qb\"})\n",
    ")\n",
    "base = base.merge(qb, on=group_cols_full, how=\"left\")\n",
    "base[\"dist_qb\"] = np.hypot(\n",
    "    base[\"x\"] - base[\"x_qb\"],\n",
    "    base[\"y\"] - base[\"y_qb\"]\n",
    ")\n",
    "\n",
    "# 2.3 Targeted receiver distance\n",
    "def add_dist_to_targeted_receiver(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tr = (\n",
    "        df[df[\"player_role\"] == \"Targeted Receiver\"]\n",
    "        [group_cols_full + [\"x\", \"y\"]]\n",
    "        .rename(columns={\"x\": \"x_tr\", \"y\": \"y_tr\"})\n",
    "    )\n",
    "    df = df.merge(tr, on=group_cols_full, how=\"left\")\n",
    "    df[\"dist_to_tr\"] = np.hypot(\n",
    "        df[\"x\"] - df[\"x_tr\"],\n",
    "        df[\"y\"] - df[\"y_tr\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "base = add_dist_to_targeted_receiver(base)\n",
    "\n",
    "# 2.4 nearest opponent\n",
    "def nearest_opponent_distance(df: pd.DataFrame, side_col: str = \"player_side\") -> pd.DataFrame:\n",
    "    off  = df[df[side_col] == \"Offense\"]\n",
    "    deff = df[df[side_col] == \"Defense\"]\n",
    "\n",
    "    off_m = off.merge(\n",
    "        deff[[\"game_id\", \"play_id\", \"frame_id\", \"nfl_id\", \"x\", \"y\"]],\n",
    "        on=[\"game_id\", \"play_id\", \"frame_id\"],\n",
    "        suffixes=(\"\", \"_opp\")\n",
    "    )\n",
    "    off_m[\"dist_opp\"] = np.hypot(\n",
    "        off_m[\"x\"] - off_m[\"x_opp\"],\n",
    "        off_m[\"y\"] - off_m[\"y_opp\"]\n",
    "    )\n",
    "    off_min = (\n",
    "        off_m.groupby(player_key)[\"dist_opp\"]\n",
    "             .min()\n",
    "             .reset_index()\n",
    "    )\n",
    "\n",
    "    deff_m = deff.merge(\n",
    "        off[[\"game_id\", \"play_id\", \"frame_id\", \"nfl_id\", \"x\", \"y\"]],\n",
    "        on=[\"game_id\", \"play_id\", \"frame_id\"],\n",
    "        suffixes=(\"\", \"_opp\")\n",
    "    )\n",
    "    deff_m[\"dist_opp\"] = np.hypot(\n",
    "        deff_m[\"x\"] - deff_m[\"x_opp\"],\n",
    "        deff_m[\"y\"] - deff_m[\"y_opp\"]\n",
    "    )\n",
    "    deff_min = (\n",
    "        deff_m.groupby(player_key)[\"dist_opp\"]\n",
    "              .min()\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "    both = pd.concat([off_min, deff_min], ignore_index=True)\n",
    "    return both\n",
    "\n",
    "nearest = nearest_opponent_distance(base)\n",
    "base = base.merge(nearest, on=player_key, how=\"left\")\n",
    "base.rename(columns={\"dist_opp\": \"dist_nearest_opp\"}, inplace=True)\n",
    "\n",
    "# one-hot side/role\n",
    "base = pd.get_dummies(\n",
    "    base,\n",
    "    columns=[\"player_side\", \"player_role\"],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# keep only players to predict\n",
    "seq_df = base[base[\"player_to_predict\"]].copy()\n",
    "seq_df = seq_df.sort_values(player_key)\n",
    "\n",
    "# restrict outputs to those players\n",
    "keys_pp = seq_df[[\"game_id\", \"play_id\", \"nfl_id\"]].drop_duplicates()\n",
    "df_out_pp = df_output.merge(\n",
    "    keys_pp,\n",
    "    on=[\"game_id\", \"play_id\", \"nfl_id\"],\n",
    "    how=\"inner\"\n",
    ").sort_values(player_key)\n",
    "\n",
    "# =========================\n",
    "# 3. Build TRAIN sequences\n",
    "# =========================\n",
    "in_lengths = seq_df.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"frame_id\"].nunique()\n",
    "out_lengths = df_out_pp.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"frame_id\"].max()\n",
    "\n",
    "T_IN   = 10\n",
    "MAX_OUT = 20\n",
    "\n",
    "frame_features = [\n",
    "    \"s\", \"a\", \"o\", \"dir\",\n",
    "    \"vx\", \"vy\",\n",
    "    \"dx\", \"dy\", \"ddx\", \"ddy\",\n",
    "    \"dist_to_ball\",\n",
    "    \"ball_dirx\", \"ball_diry\",\n",
    "    \"x_qb\", \"y_qb\", \"dist_qb\",\n",
    "    \"dist_nearest_opp\",\n",
    "    \"dist_to_tr\",\n",
    "    \"num_frames_output\",\n",
    "]\n",
    "\n",
    "dummy_cols = [\n",
    "    c for c in seq_df.columns\n",
    "    if c.startswith(\"player_side_\") or c.startswith(\"player_role_\")\n",
    "]\n",
    "frame_features = frame_features + dummy_cols\n",
    "\n",
    "X_list   = []\n",
    "Y_list   = []\n",
    "start_xy = []\n",
    "lengths  = []\n",
    "meta     = []\n",
    "\n",
    "for (g, p, n), g_in in seq_df.groupby([\"game_id\", \"play_id\", \"nfl_id\"]):\n",
    "    g_out = df_out_pp[\n",
    "        (df_out_pp[\"game_id\"] == g) &\n",
    "        (df_out_pp[\"play_id\"] == p) &\n",
    "        (df_out_pp[\"nfl_id\"] == n)\n",
    "    ].sort_values(\"frame_id\")\n",
    "\n",
    "    if len(g_in) < T_IN or len(g_out) < 1:\n",
    "        continue\n",
    "\n",
    "    g_in_last = g_in.tail(T_IN).copy()\n",
    "    actual_len = min(len(g_out), MAX_OUT)\n",
    "    g_out_first = g_out.head(actual_len).copy()\n",
    "\n",
    "    x0, y0 = g_in_last.iloc[-1][[\"x\", \"y\"]]\n",
    "    start_xy.append([x0, y0])\n",
    "\n",
    "    g_in_last[\"x_rel\"] = g_in_last[\"x\"] - x0\n",
    "    g_in_last[\"y_rel\"] = g_in_last[\"y\"] - y0\n",
    "\n",
    "    feat_df = g_in_last[frame_features + [\"x_rel\", \"y_rel\"]]\n",
    "    if feat_df.isna().any().any():\n",
    "        start_xy.pop()\n",
    "        continue\n",
    "\n",
    "    Xin = feat_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "    Y_abs = g_out_first[[\"x\", \"y\"]].to_numpy(dtype=np.float32)\n",
    "    Y_rel = Y_abs - np.array([[x0, y0]], dtype=np.float32)\n",
    "\n",
    "    if actual_len < MAX_OUT:\n",
    "        padding = np.zeros((MAX_OUT - actual_len, 2), dtype=np.float32)\n",
    "        Y_rel = np.vstack([Y_rel, padding])\n",
    "\n",
    "    X_list.append(Xin)\n",
    "    Y_list.append(Y_rel)\n",
    "    lengths.append(actual_len)\n",
    "    side = \"Offense\" if g_in[\"player_side_Offense\"].iloc[0] == 1 else \"Defense\"\n",
    "    meta.append((g, p, n, side))\n",
    "\n",
    "X_seq    = np.stack(X_list)\n",
    "Y_seq    = np.stack(Y_list)\n",
    "start_xy = np.array(start_xy, np.float32)\n",
    "lengths  = np.array(lengths, np.int32)\n",
    "\n",
    "print(\"X_seq:\", X_seq.shape, \"Y_seq:\", Y_seq.shape)\n",
    "\n",
    "# =========================\n",
    "# 4. Train/val split + scaling\n",
    "# =========================\n",
    "N, T_IN_, F = X_seq.shape\n",
    "_, T_OUT_, _ = Y_seq.shape\n",
    "print(\"T_IN, F =\", T_IN_, F, \"| T_OUT =\", T_OUT_)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val, start_train, start_val, len_train, len_val = train_test_split(\n",
    "    X_seq, Y_seq, start_xy, lengths, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "meta_arr = np.array(meta, dtype=object)\n",
    "_, meta_val = train_test_split(meta_arr, test_size=0.2, random_state=42)\n",
    "player_side_val = np.array([m[3] for m in meta_val])\n",
    "\n",
    "X_train_flat = X_train.reshape(-1, F)\n",
    "feat_mean = X_train_flat.mean(axis=0)\n",
    "feat_std  = X_train_flat.std(axis=0) + 1e-6\n",
    "\n",
    "def scale_X(X):\n",
    "    return (X - feat_mean) / feat_std\n",
    "\n",
    "X_train_sc = scale_X(X_train)\n",
    "X_val_sc   = scale_X(X_val)\n",
    "Y_train_sc = Y_train\n",
    "Y_val_sc   = Y_val\n",
    "\n",
    "# =========================\n",
    "# 5. Define & train TCN\n",
    "# =========================\n",
    "class MaskedMSE(keras.losses.Loss):\n",
    "    def __init__(self, name=\"masked_mse\"):\n",
    "        super().__init__(name=name)\n",
    "    def call(self, y_true, y_pred):\n",
    "        squared_diff = tf.square(y_true - y_pred)\n",
    "        mask = tf.cast(\n",
    "            tf.logical_or(\n",
    "                tf.abs(y_true[:, :, 0]) > 1e-6,\n",
    "                tf.abs(y_true[:, :, 1]) > 1e-6,\n",
    "            ),\n",
    "            tf.float32,\n",
    "        )\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        masked_loss = squared_diff * mask\n",
    "        sum_loss   = tf.reduce_sum(masked_loss)\n",
    "        num_valid  = tf.reduce_sum(mask) + 1e-8\n",
    "        return sum_loss / num_valid\n",
    "\n",
    "inputs = keras.Input(shape=(T_IN, F))\n",
    "x = TCN(\n",
    "    nb_filters=64,\n",
    "    kernel_size=3,\n",
    "    dilations=[1, 2, 4, 8],\n",
    "    dropout_rate=0.1,\n",
    "    return_sequences=False,\n",
    ")(inputs)\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = keras.layers.Dense(MAX_OUT * 2)(x)\n",
    "outputs = keras.layers.Reshape((MAX_OUT, 2))(x)\n",
    "\n",
    "tcn_model = keras.Model(inputs, outputs)\n",
    "tcn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=MaskedMSE(),\n",
    ")\n",
    "\n",
    "history = tcn_model.fit(\n",
    "    X_train_sc, Y_train_sc,\n",
    "    validation_data=(X_val_sc, Y_val_sc),\n",
    "    epochs=12,        # fewer epochs on Kaggle for speed\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5b. VALIDATION RMSE (same as in VS Code)\n",
    "# =========================\n",
    "\n",
    "# Predictions on validation set\n",
    "Y_val_pred = tcn_model.predict(X_val_sc, verbose=0)  # (N_val, MAX_OUT, 2), relative coords\n",
    "Y_val_true = Y_val_sc                                # (N_val, MAX_OUT, 2)\n",
    "\n",
    "def rmse_masked(y_true, y_pred, lengths):\n",
    "    \"\"\"RMSE over valid frames only (uses per-sequence length).\"\"\"\n",
    "    total_sq_dist = 0.0\n",
    "    total_frames  = 0\n",
    "    for i in range(len(lengths)):\n",
    "        L = lengths[i]\n",
    "        diff    = y_true[i, :L] - y_pred[i, :L]   # (L, 2)\n",
    "        sq_dist = np.sum(diff**2, axis=-1)        # dx^2 + dy^2 per frame\n",
    "        total_sq_dist += sq_dist.sum()\n",
    "        total_frames  += L\n",
    "    return np.sqrt(total_sq_dist / total_frames)\n",
    "\n",
    "# Overall RMSE in relative coords\n",
    "rmse_rel = rmse_masked(Y_val_true, Y_val_pred, len_val)\n",
    "print(f\"TCN RMSE (masked, relative) on val: {rmse_rel:.3f}\")\n",
    "\n",
    "# RMSE in absolute field coordinates (like in VSC)\n",
    "Y_val_true_abs = start_val[:, None, :] + Y_val_true\n",
    "Y_val_pred_abs = start_val[:, None, :] + Y_val_pred\n",
    "rmse_abs = rmse_masked(Y_val_true_abs, Y_val_pred_abs, len_val)\n",
    "print(f\"TCN RMSE (masked, absolute) on val: {rmse_abs:.3f}\")\n",
    "\n",
    "# Offense / Defense separate (optional, same as before)\n",
    "off_idx = np.where(player_side_val == \"Offense\")[0]\n",
    "def_idx = np.where(player_side_val == \"Defense\")[0]\n",
    "\n",
    "rmse_offense = rmse_masked(\n",
    "    Y_val_true[off_idx],\n",
    "    Y_val_pred[off_idx],\n",
    "    len_val[off_idx],\n",
    ")\n",
    "rmse_defense = rmse_masked(\n",
    "    Y_val_true[def_idx],\n",
    "    Y_val_pred[def_idx],\n",
    "    len_val[def_idx],\n",
    ")\n",
    "\n",
    "print(f\"RMSE (Offense, abs): {rmse_masked(Y_val_true_abs[off_idx], Y_val_pred_abs[off_idx], len_val[off_idx]):.3f}\")\n",
    "print(f\"RMSE (Defense, abs): {rmse_masked(Y_val_true_abs[def_idx], Y_val_pred_abs[def_idx], len_val[def_idx]):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00f3321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:30:31.163916Z",
     "iopub.status.busy": "2025-12-03T06:30:31.163705Z",
     "iopub.status.idle": "2025-12-03T06:30:52.621304Z",
     "shell.execute_reply": "2025-12-03T06:30:52.620566Z"
    },
    "id": "vjtysL5Hggs6",
    "papermill": {
     "duration": 21.47311,
     "end_time": "2025-12-03T06:30:52.622823",
     "exception": false,
     "start_time": "2025-12-03T06:30:31.149713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- helper: build test sequences from test_input ----------\n",
    "def build_test_sequences_from_input(test_input_pd: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Given test_input (pandas), build:\n",
    "      - X_test_seq : (N_test, T_IN, F)\n",
    "      - start_xy_test : (N_test, 2)\n",
    "      - keys_to_row : mapping (game_id, play_id, nfl_id) -> row index\n",
    "    using the SAME feature-engineering as in training.\n",
    "    \"\"\"\n",
    "    test_inp = standardize_by_play_direction(test_input_pd.copy())\n",
    "\n",
    "    group_cols_full  = [\"game_id\", \"play_id\", \"frame_id\"]\n",
    "    player_key       = [\"game_id\", \"play_id\", \"frame_id\", \"nfl_id\"]\n",
    "\n",
    "    base_test = test_inp.sort_values(player_key)\n",
    "\n",
    "    # self-based\n",
    "    base_test[[\"dx\", \"dy\"]] = (\n",
    "        base_test.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[[\"x\", \"y\"]]\n",
    "                .diff()\n",
    "                .fillna(0.0)\n",
    "    )\n",
    "    base_test[\"do\"] = (\n",
    "        base_test.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"o\"]\n",
    "                .diff()\n",
    "                .fillna(0.0)\n",
    "    )\n",
    "    base_test[\"dist_to_ball\"] = np.hypot(\n",
    "        base_test[\"x\"] - base_test[\"ball_land_x\"],\n",
    "        base_test[\"y\"] - base_test[\"ball_land_y\"]\n",
    "    )\n",
    "    angle_to_ball_test = np.arctan2(\n",
    "        base_test[\"ball_land_y\"] - base_test[\"y\"],\n",
    "        base_test[\"ball_land_x\"] - base_test[\"x\"]\n",
    "    )\n",
    "    base_test[\"ball_dirx\"] = np.cos(angle_to_ball_test)\n",
    "    base_test[\"ball_diry\"] = np.sin(angle_to_ball_test)\n",
    "    rad_dir_test = np.deg2rad(base_test[\"dir\"])\n",
    "    base_test[\"vx\"] = base_test[\"s\"] * np.cos(rad_dir_test)\n",
    "    base_test[\"vy\"] = base_test[\"s\"] * np.sin(rad_dir_test)\n",
    "    base_test[[\"ddx\", \"ddy\"]] = (\n",
    "        base_test.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[[\"dx\", \"dy\"]]\n",
    "                .diff()\n",
    "                .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    # QB\n",
    "    qb_test = (\n",
    "        base_test[base_test[\"player_role\"] == \"Passer\"]\n",
    "        [[\"game_id\", \"play_id\", \"frame_id\", \"x\", \"y\"]]\n",
    "        .rename(columns={\"x\": \"x_qb\", \"y\": \"y_qb\"})\n",
    "    )\n",
    "    base_test = base_test.merge(qb_test, on=[\"game_id\", \"play_id\", \"frame_id\"], how=\"left\")\n",
    "    base_test[\"dist_qb\"] = np.hypot(\n",
    "        base_test[\"x\"] - base_test[\"x_qb\"],\n",
    "        base_test[\"y\"] - base_test[\"y_qb\"]\n",
    "    )\n",
    "\n",
    "    # targeted receiver\n",
    "    base_test = add_dist_to_targeted_receiver(base_test)\n",
    "\n",
    "    # nearest opponent\n",
    "    nearest_test = nearest_opponent_distance(base_test)\n",
    "    base_test = base_test.merge(nearest_test, on=player_key, how=\"left\")\n",
    "    base_test.rename(columns={\"dist_opp\": \"dist_nearest_opp\"}, inplace=True)\n",
    "\n",
    "    # one-hot\n",
    "    base_test = pd.get_dummies(\n",
    "        base_test,\n",
    "        columns=[\"player_side\", \"player_role\"],\n",
    "        drop_first=True\n",
    "    )\n",
    "\n",
    "    # ensure SAME dummy columns as in train\n",
    "    for col in dummy_cols:\n",
    "        if col not in base_test.columns:\n",
    "            base_test[col] = 0.0\n",
    "\n",
    "    seq_df_test = base_test[base_test[\"player_to_predict\"]].copy()\n",
    "    seq_df_test = seq_df_test.sort_values(player_key)\n",
    "\n",
    "    X_list_test   = []\n",
    "    start_xy_test = []\n",
    "    meta_test     = []\n",
    "\n",
    "    for (g, p, n), g_in in seq_df_test.groupby([\"game_id\", \"play_id\", \"nfl_id\"]):\n",
    "        if len(g_in) < T_IN:\n",
    "            continue\n",
    "\n",
    "        g_in_last = g_in.tail(T_IN).copy()\n",
    "        x0, y0 = g_in_last.iloc[-1][[\"x\", \"y\"]]\n",
    "        start_xy_test.append([x0, y0])\n",
    "\n",
    "        g_in_last[\"x_rel\"] = g_in_last[\"x\"] - x0\n",
    "        g_in_last[\"y_rel\"] = g_in_last[\"y\"] - y0\n",
    "\n",
    "        # --- NEW: guarantee all frame_features exist ---\n",
    "        for col in frame_features:\n",
    "            if col not in g_in_last.columns:\n",
    "                g_in_last[col] = 0.0\n",
    "\n",
    "        feat_df_test = g_in_last[frame_features + [\"x_rel\", \"y_rel\"]]\n",
    "\n",
    "        # still skip NaN rows if something went wrong\n",
    "        if feat_df_test.isna().any().any():\n",
    "            start_xy_test.pop()\n",
    "            continue\n",
    "\n",
    "        Xin_test = feat_df_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "        X_list_test.append(Xin_test)\n",
    "        meta_test.append((g, p, n))\n",
    "\n",
    "\n",
    "    # SAFE GUARD: no valid sequences at all\n",
    "    if len(X_list_test) == 0:\n",
    "        return None, None, {}\n",
    "\n",
    "    X_test_seq    = np.stack(X_list_test)\n",
    "    start_xy_test = np.array(start_xy_test, np.float32)\n",
    "\n",
    "    keys_to_row = {key: i for i, key in enumerate(meta_test)}\n",
    "\n",
    "    return X_test_seq, start_xy_test, keys_to_row\n",
    "\n",
    "\n",
    "def predict(test, test_input):\n",
    "    \"\"\"\n",
    "    Called by Kaggle on the (hidden) test set.\n",
    "    Must return a DataFrame with columns ['x', 'y'] in the SAME ROW ORDER as `test`.\n",
    "    \"\"\"\n",
    "    # Convert to pandas if needed (polars, etc.)\n",
    "    if hasattr(test, \"to_pandas\"):\n",
    "        test = test.to_pandas()\n",
    "    if hasattr(test_input, \"to_pandas\"):\n",
    "        test_input = test_input.to_pandas()\n",
    "\n",
    "    # ---- NEW: map (game_id, play_id) -> play_direction (original) ----\n",
    "    dir_map = (\n",
    "        test_input[[\"game_id\", \"play_id\", \"play_direction\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index([\"game_id\", \"play_id\"])[\"play_direction\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # Build sequences\n",
    "    \n",
    "    X_test_seq, start_xy_test, keys_to_row = build_test_sequences_from_input(test_input)\n",
    "\n",
    "    # If NO sequences could be built for this chunk,\n",
    "    # return zeros but keep the right length.\n",
    "    if X_test_seq is None or X_test_seq.shape[0] == 0:\n",
    "        return pd.DataFrame({\n",
    "            \"x\": np.zeros(len(test), dtype=np.float32),\n",
    "            \"y\": np.zeros(len(test), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "    # Scale + predict (using the trained model)\n",
    "    X_test_sc  = scale_X(X_test_seq)\n",
    "    X_test_sc_clean = np.nan_to_num(X_test_sc)\n",
    "    batch_size = 1000\n",
    "    num_samples = X_test_sc_clean.shape[0]\n",
    "    Y_test_rel = []\n",
    "\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        batch = X_test_sc_clean[start:end]\n",
    "        preds = tcn_model.predict(batch, verbose=0)\n",
    "        Y_test_rel.append(preds)\n",
    "\n",
    "    Y_test_rel = np.concatenate(Y_test_rel, axis=0)\n",
    "\n",
    "    Y_test_abs = start_xy_test[:, None, :] + Y_test_rel   # (N_test, MAX_OUT, 2)\n",
    "\n",
    "    # Map back row-by-row\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for _, row in test.iterrows():\n",
    "        g = row[\"game_id\"]\n",
    "        p = row[\"play_id\"]\n",
    "        n = row[\"nfl_id\"]\n",
    "        f = row[\"frame_id\"] - 1  # 0-based\n",
    "\n",
    "        key = (g, p, n)\n",
    "        if key not in keys_to_row:\n",
    "            xs.append(0.0)\n",
    "            ys.append(0.0)\n",
    "            continue\n",
    "\n",
    "        idx  = keys_to_row[key]\n",
    "        traj = Y_test_abs[idx]   # (MAX_OUT, 2) in STANDARDIZED coords\n",
    "\n",
    "        # pick the frame we want in standardized coordinates\n",
    "        if f < traj.shape[0]:\n",
    "            x_pred = float(traj[f, 0])\n",
    "            y_pred = float(traj[f, 1])\n",
    "        else:\n",
    "            # For frames beyond MAX_OUT, repeat last position\n",
    "            x_pred = float(traj[-1, 0])\n",
    "            y_pred = float(traj[-1, 1])\n",
    "\n",
    "        # ---- NEW: de-standardize for original left plays ----\n",
    "        play_dir = dir_map.get((g, p), \"right\")\n",
    "        if play_dir == \"left\":\n",
    "            # invert the same transform we used when standardizing\n",
    "            x_pred = 120.0 - x_pred\n",
    "            y_pred = 53.3 - y_pred\n",
    "\n",
    "        xs.append(x_pred)\n",
    "        ys.append(y_pred)\n",
    "\n",
    "    preds = pd.DataFrame({\"x\": xs, \"y\": ys})\n",
    "    assert len(preds) == len(test)\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------- START THE EVALUATION SERVER (MUST BE AT GLOBAL LEVEL) ---------\n",
    "import kaggle_evaluation.nfl_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # This is what Kaggle uses on the hidden test set\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # This lets you test locally inside the notebook, using the public train/test files\n",
    "    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    },
    {
     "datasetId": 8881582,
     "sourceId": 13936392,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8885036,
     "sourceId": 13941283,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 326.98422,
   "end_time": "2025-12-03T06:30:56.286790",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-03T06:25:29.302570",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
